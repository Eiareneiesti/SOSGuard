<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Real-time Audio Classification</title>

    <style>
        #results {
            font-family: monospace;
            white-space: pre;
        }
    </style>

    <!-- Include the Edge Impulse SDK files -->
    <script src="path/to/edge-impulse-standalone.js"></script>
    <script src="path/to/run-impulse.js"></script>
</head>
<body>
    <h1></h1>

    <p>
        <button id="start-button">Start Recording</button>
        <button id="stop-button" disabled>Stop Recording</button>
    </p>
    <p id="results"></p>

    <script>
        // Your JavaScript code for real-time audio classification
        (async () => {
            // Initialize the classifier
            var classifier = new EdgeImpulseClassifier();
            await classifier.init();

            // Get project info and display it
            let project = classifier.getProjectInfo();
            document.querySelector('h1').textContent = project.owner + ' / ' + project.name + ' (version ' + project.deploy_version + ')';

            // Audio context for recording
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            let mediaRecorder;
            let audioChunks = [];

            // Function to start recording audio
            async function startRecording() {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    mediaRecorder = new MediaRecorder(stream);
                    mediaRecorder.ondataavailable = handleDataAvailable;
                    mediaRecorder.start();
                    document.getElementById('start-button').disabled = true;
                    document.getElementById('stop-button').disabled = false;
                } catch (err) {
                    console.error('Error accessing microphone:', err);
                }
            }

            // Function to handle audio data
            function handleDataAvailable(event) {
                audioChunks.push(event.data);
            }

            // Function to stop recording and extract features
            function stopRecording() {
                mediaRecorder.stop();
                document.getElementById('start-button').disabled = false;
                document.getElementById('stop-button').disabled = true;

                const audioBlob = new Blob(audioChunks);
                const audioUrl = URL.createObjectURL(audioBlob);

                const audioElement = new Audio(audioUrl);
                audioElement.controls = true;
                document.body.appendChild(audioElement);

                // Extract features from the recorded audio
                extractFeaturesFromAudio(audioUrl);
            }

            // Function to extract features from audio
            async function extractFeaturesFromAudio(audioUrl) {
                try {
                    const audioBuffer = await fetch(audioUrl)
                        .then(response => response.arrayBuffer())
                        .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer));

                    const audioData = audioBuffer.getChannelData(0); // Assuming mono audio
                    const res = classifier.classify(audioData);
                    document.querySelector('#results').textContent = JSON.stringify(res, null, 4);
                } catch (ex) {
                    console.error('Failed to classify:', ex);
                }
            }

            // Event listeners for buttons
            document.getElementById('start-button').addEventListener('click', startRecording);
            document.getElementById('stop-button').addEventListener('click', stopRecording);
        })();
    </script>
</body>
</html>
